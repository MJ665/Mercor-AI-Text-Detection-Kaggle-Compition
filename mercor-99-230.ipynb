{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117171,"databundleVersionId":14089262,"sourceType":"competition"}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from wordcloud import WordCloud\n# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n# from sklearn.pipeline import Pipeline\n# from scipy.stats import uniform\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n# from datasets import Dataset\n# import torch\n\n# # Load the data\n\n# # Load the data\n# try:\n#     train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv')\n#     test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n#     sample_submission_df = pd.read_csv('/kaggle/input/mercor-ai-detection/sample_submission.csv')\n# except FileNotFoundError:\n#     print(\"Ensure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n#     # Create dummy dataframes for demonstration if files are not found\n#     train_df = pd.DataFrame({\n#         'id': range(100),\n#         'topic': ['topic_' + str(i % 5) for i in range(100)],\n#         'answer': ['This is a sample answer ' * 10 for i in range(100)],\n#         'is_cheating': np.random.randint(0, 2, 100)\n#     })\n#     test_df = pd.DataFrame({\n#         'id': range(100),\n#         'topic': ['topic_' + str(i % 5) for i in range(100)],\n#         'answer': ['This is a sample test answer ' * 10 for i in range(100)]\n#     })\n\n# # --- 1. Exploratory Data Analysis (EDA) ---\n# print(\"Performing Exploratory Data Analysis...\")\n\n# # Plot 1: Distribution of Target Variable\n# plt.figure(figsize=(6, 6))\n# train_df['is_cheating'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=['skyblue', 'salmon'])\n# plt.title('Distribution of Target Variable (is_cheating)')\n# plt.ylabel('')\n# plt.show()\n\n# # Plot 2: Text Length Analysis\n# train_df['answer_length'] = train_df['answer'].str.len()\n# plt.figure(figsize=(10, 6))\n# sns.histplot(data=train_df, x='answer_length', hue='is_cheating', kde=True, bins=50)\n# plt.title('Distribution of Answer Length by Class')\n# plt.xlabel('Answer Length')\n# plt.ylabel('Frequency')\n# plt.show()\n\n# # Plot 3: Topic Distribution\n# plt.figure(figsize=(12, 6))\n# sns.countplot(y='topic', data=train_df, order=train_df['topic'].value_counts().index, palette='viridis')\n# plt.title('Distribution of Topics')\n# plt.xlabel('Count')\n# plt.ylabel('Topic')\n# plt.show()\n\n# # Plot 4 & 5: Word Clouds\n# cheating_text = ' '.join(train_df[train_df['is_cheating'] == 1]['answer'].dropna())\n# non_cheating_text = ' '.join(train_df[train_df['is_cheating'] == 0]['answer'].dropna())\n\n# wordcloud_cheating = WordCloud(width=800, height=400, background_color='white').generate(cheating_text)\n# wordcloud_non_cheating = WordCloud(width=800, height=400, background_color='white').generate(non_cheating_text)\n\n# fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n# axes[0].imshow(wordcloud_cheating, interpolation='bilinear')\n# axes[0].set_title('Word Cloud for Cheating Class')\n# axes[0].axis('off')\n# axes[1].imshow(wordcloud_non_cheating, interpolation='bilinear')\n# axes[1].set_title('Word Cloud for Non-Cheating Class')\n# axes[1].axis('off')\n# plt.show()\n\n# # Data Preprocessing for Models\n# X = train_df['answer']\n# y = train_df['is_cheating']\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# # --- 2. Modeling ---\n# print(\"Training and evaluating models...\")\n# results = {}\n\n# # --- Model 1: Logistic Regression with TF-IDF and RandomizedSearchCV ---\n# print(\"Training Logistic Regression with TF-IDF...\")\n# pipeline = Pipeline([\n#     ('tfidf', TfidfVectorizer()),\n#     ('clf', LogisticRegression(solver='liblinear', random_state=42))\n# ])\n\n# param_dist = {\n#     'tfidf__ngram_range': [(1, 1), (1, 2)],\n#     'tfidf__max_df': uniform(0.5, 0.5),\n#     'tfidf__min_df': range(1, 11),\n#     'clf__C': uniform(0.1, 10),\n#     'clf__penalty': ['l1', 'l2']\n# }\n\n# random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5,\n#                                    scoring='roc_auc', random_state=42, n_jobs=-1)\n# random_search.fit(X_train, y_train)\n\n# best_lr = random_search.best_estimator_\n# y_pred_lr = best_lr.predict_proba(X_val)[:, 1]\n# roc_auc_lr = roc_auc_score(y_val, y_pred_lr)\n# results['Logistic Regression'] = {'roc_auc': roc_auc_lr, 'y_pred': y_pred_lr}\n# print(f\"Logistic Regression ROC-AUC: {roc_auc_lr:.4f}\")\n\n# # Plot 6: Logistic Regression ROC Curve and Confusion Matrix\n# fpr_lr, tpr_lr, _ = roc_curve(y_val, y_pred_lr)\n# cm_lr = confusion_matrix(y_val, (y_pred_lr > 0.5).astype(int))\n\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# axes[0].plot(fpr_lr, tpr_lr, label=f'ROC curve (area = {roc_auc_lr:.2f})')\n# axes[0].plot([0, 1], [0, 1], 'k--')\n# axes[0].set_xlim([0.0, 1.0])\n# axes[0].set_ylim([0.0, 1.05])\n# axes[0].set_xlabel('False Positive Rate')\n# axes[0].set_ylabel('True Positive Rate')\n# axes[0].set_title('Logistic Regression ROC Curve')\n# axes[0].legend(loc=\"lower right\")\n\n# ConfusionMatrixDisplay(confusion_matrix=cm_lr).plot(ax=axes[1], cmap='Blues')\n# axes[1].set_title('Logistic Regression Confusion Matrix')\n# plt.show()\n\n# # --- Transformer Models ---\n# def train_transformer(model_name, train_texts, train_labels, val_texts, val_labels):\n#     train_df_hf = pd.DataFrame({'text': train_texts.tolist(), 'label': train_labels.tolist()})\n#     val_df_hf = pd.DataFrame({'text': val_texts.tolist(), 'label': val_labels.tolist()})\n\n#     train_dataset = Dataset.from_pandas(train_df_hf)\n#     val_dataset = Dataset.from_pandas(val_df_hf)\n\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n#     def tokenize_function(examples):\n#         return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n\n#     tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n#     tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n\n#     model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n#     training_args = TrainingArguments(\n#         output_dir=f'./results_{model_name.replace(\"/\", \"_\")}',\n#         evaluation_strategy=\"epoch\",\n#         learning_rate=2e-5,\n#         per_device_train_batch_size=8,\n#         per_device_eval_batch_size=8,\n#         num_train_epochs=1, # Using 1 epoch for demonstration\n#         weight_decay=0.01,\n#         logging_dir='./logs',\n#     )\n\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=tokenized_train_dataset,\n#         eval_dataset=tokenized_val_dataset,\n#     )\n\n#     trainer.train()\n\n#     predictions = trainer.predict(tokenized_val_dataset)\n#     probs = torch.softmax(torch.from_numpy(predictions.predictions), dim=-1)[:, 1].numpy()\n#     return probs\n\n# transformer_models = {\n#     'BERT': 'bert-base-uncased',\n#     'RoBERTa': 'roberta-base',\n#     'DistilBERT': 'distilbert-base-uncased'\n# }\n\n# for model_name, model_path in transformer_models.items():\n#     print(f\"Training {model_name}...\")\n#     try:\n#         y_pred_transformer = train_transformer(model_path, X_train, y_train, X_val, y_val)\n#         roc_auc_transformer = roc_auc_score(y_val, y_pred_transformer)\n#         results[model_name] = {'roc_auc': roc_auc_transformer, 'y_pred': y_pred_transformer}\n#         print(f\"{model_name} ROC-AUC: {roc_auc_transformer:.4f}\")\n\n#         # Plot ROC Curve and Confusion Matrix for each transformer model\n#         fpr_transformer, tpr_transformer, _ = roc_curve(y_val, y_pred_transformer)\n#         cm_transformer = confusion_matrix(y_val, (y_pred_transformer > 0.5).astype(int))\n\n#         fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n#         axes[0].plot(fpr_transformer, tpr_transformer, label=f'ROC curve (area = {roc_auc_transformer:.2f})')\n#         axes[0].plot([0, 1], [0, 1], 'k--')\n#         axes[0].set_xlim([0.0, 1.0])\n#         axes[0].set_ylim([0.0, 1.05])\n#         axes[0].set_xlabel('False Positive Rate')\n#         axes[0].set_ylabel('True Positive Rate')\n#         axes[0].set_title(f'{model_name} ROC Curve')\n#         axes[0].legend(loc=\"lower right\")\n\n#         ConfusionMatrixDisplay(confusion_matrix=cm_transformer).plot(ax=axes[1], cmap='Blues')\n#         axes[1].set_title(f'{model_name} Confusion Matrix')\n#         plt.show()\n\n#     except Exception as e:\n#         print(f\"Error training {model_name}: {e}\")\n#         print(f\"Skipping {model_name} due to the error.\")\n\n# # --- 3. Model Comparison ---\n# print(\"Comparing model performance...\")\n\n# # Plot 7: Comparison of ROC-AUC Scores\n# model_names = list(results.keys())\n# roc_auc_scores = [res['roc_auc'] for res in results.values()]\n\n# plt.figure(figsize=(10, 6))\n# sns.barplot(x=model_names, y=roc_auc_scores, palette='mako')\n# plt.title('Comparison of Model ROC-AUC Scores')\n# plt.xlabel('Model')\n# plt.ylabel('ROC-AUC Score')\n# plt.ylim(0.5, 1.0)\n# for i, score in enumerate(roc_auc_scores):\n#     plt.text(i, score + 0.01, f'{score:.4f}', ha='center')\n# plt.show()\n\n# # Plot 8: Combined ROC Curves\n# plt.figure(figsize=(10, 8))\n# for model_name, result in results.items():\n#     fpr, tpr, _ = roc_curve(y_val, result['y_pred'])\n#     plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {result['roc_auc']:.4f})\")\n# plt.plot([0, 1], [0, 1], 'k--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Combined ROC Curves of All Models')\n# plt.legend(loc=\"lower right\")\n# plt.show()\n\n\n# # --- 4. Submission ---\n# print(\"Generating submission file...\")\n# if results:\n#     best_model_name = max(results, key=lambda k: results[k]['roc_auc'])\n#     print(f\"Best performing model: {best_model_name}\")\n\n#     if 'Logistic Regression' in best_model_name:\n#         test_pred_probs = best_lr.predict_proba(test_df['answer'])[:, 1]\n#     else:\n#         # For a real submission, the best transformer model should be retrained on the full training data.\n#         # Here, we'll just use the validation model for prediction as a demonstration.\n#         test_texts = test_df['answer'].tolist()\n#         # This part requires re-running the prediction on the test set with the best transformer model.\n#         # For simplicity, we are creating a dummy prediction here.\n#         test_pred_probs = np.random.rand(len(test_df))\n#         print(\"Note: For a real submission with a transformer, retrain on full data.\")\n\n\n#     submission_df = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_pred_probs})\n#     # --- 4. Submission ---\n#     submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n#     print(\"Submission file '/kaggle/working/submission.csv' created successfully.\")\n\n#     print(\"Submission file 'submission.csv' created successfully.\")\n\n#     # Plot 9: Distribution of Predicted Probabilities on Test Set\n#     plt.figure(figsize=(10, 6))\n#     sns.histplot(test_pred_probs, bins=50, kde=True)\n#     plt.title(f'Distribution of Predicted Probabilities on Test Set ({best_model_name})')\n#     plt.xlabel('Predicted Probability')\n#     plt.ylabel('Frequency')\n#     plt.show()\n# else:\n#     print(\"No models were successfully trained. Submission file not generated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:21:52.423764Z","iopub.execute_input":"2025-11-12T19:21:52.424079Z","iopub.status.idle":"2025-11-12T19:23:03.851832Z","shell.execute_reply.started":"2025-11-12T19:21:52.424054Z","shell.execute_reply":"2025-11-12T19:23:03.850577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run in a cell\n!python --version\nimport sys, subprocess\nprint(\"sys.version:\", sys.version)\n!pip show torch || true\n!pip show torch_xla || true\n!pip install cloud-tpu-client==0.10\n!pip install torch==2.0.0 torchvision==0.15.1 \\\n  https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:17.141751Z","iopub.execute_input":"2025-11-12T19:35:17.142128Z","iopub.status.idle":"2025-11-12T19:35:23.237870Z","shell.execute_reply.started":"2025-11-12T19:35:17.142106Z","shell.execute_reply":"2025-11-12T19:35:23.236556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ===================================================================\n# # 1. SETUP AND INITIAL CONFIGURATION\n# # ===================================================================\n# import os\n# import random\n# import time\n# import warnings\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import torch\n# import torch.nn as nn\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import (roc_auc_score, confusion_matrix,\n#                              ConfusionMatrixDisplay, roc_curve, auc,\n#                              precision_recall_curve)\n# from transformers import (AutoTokenizer, AutoConfig,\n#                             AutoModelForSequenceClassification,\n#                             DataCollatorWithPadding, TrainingArguments,\n#                             Trainer, EarlyStoppingCallback,\n#                             logging as transformers_logging)\n# from datasets import Dataset\n\n# # --- Core Settings ---\n# # Input and output paths for Kaggle environment\n# DATA_DIR = \"/kaggle/input/mercor-ai-detection\"\n# OUTPUT_SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n# PLOT_DIR = \"/kaggle/working/plots\"\n# MODEL_OUTPUT_DIR = \"/kaggle/working/finetuned_model\"\n\n# # Create directories for outputs\n# os.makedirs(PLOT_DIR, exist_ok=True)\n# os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n\n# # --- Model & Training Hyperparameters ---\n# # Using a model pre-trained specifically for AI text detection\n# MODEL_NAME = \"fakespot-ai/roberta-base-ai-text-detection-v1\"\n# MAX_TOKEN_LENGTH = 512\n# TRAIN_BATCH_SIZE = 16  # Adjusted for better stability on various GPUs\n# EVAL_BATCH_SIZE = 16\n# NUM_EPOCHS = 8\n# LEARNING_RATE = 2e-5\n# SEED = 42\n\n# # --- Reproducibility ---\n# def set_seed(seed_value):\n#     \"\"\"Set seed for reproducibility.\"\"\"\n#     random.seed(seed_value)\n#     np.random.seed(seed_value)\n#     torch.manual_seed(seed_value)\n#     if torch.cuda.is_available():\n#         torch.cuda.manual_seed_all(seed_value)\n\n# set_seed(SEED)\n# warnings.filterwarnings(\"ignore\", category=UserWarning)\n# transformers_logging.set_verbosity_error()\n\n# print(\"Setup Complete. Using model:\", MODEL_NAME)\n\n# # ===================================================================\n# # 2. DATA LOADING AND PREPARATION\n# # ===================================================================\n# # --- Helper Functions for Text Cleaning ---\n# def clean_text(text):\n#     \"\"\"Removes extra whitespace, newlines, and returns empty string for NaNs.\"\"\"\n#     if pd.isna(text):\n#         return \"\"\n#     return \" \".join(str(text).strip().replace(\"\\r\", \" \").replace(\"\\n\", \" \").split())\n\n# def format_input_text(topic, answer):\n#     \"\"\"Creates a structured input string from topic and answer.\"\"\"\n#     clean_topic = clean_text(topic)\n#     clean_answer = clean_text(answer)\n#     return f\"TOPIC: {clean_topic}\\n\\nANSWER: {clean_answer}\"\n\n# # --- Load DataFrames ---\n# train_full_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n# test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n\n# # --- Preprocess and Create Input Text ---\n# train_full_df[\"text\"] = train_full_df.apply(lambda row: format_input_text(row[\"topic\"], row[\"answer\"]), axis=1)\n# train_full_df[\"label\"] = train_full_df[\"is_cheating\"].astype(int)\n\n# test_df[\"text\"] = test_df.apply(lambda row: format_input_text(row.get(\"topic\", \"\"), row.get(\"answer\", \"\")), axis=1)\n# print(f\"Data loaded. Train samples: {len(train_full_df)}, Test samples: {len(test_df)}\")\n\n# # --- Data Augmentation: Upsampling Minority Class ---\n# label_counts = train_full_df[\"label\"].value_counts()\n# print(\"Original label distribution:\\n\", label_counts)\n\n# minority_label = label_counts.idxmin()\n# majority_count = label_counts.max()\n# minority_count = label_counts.min()\n\n# if minority_count < majority_count:\n#     samples_to_add = majority_count - minority_count\n#     minority_df = train_full_df[train_full_df[\"label\"] == minority_label]\n#     upsampled_minority = minority_df.sample(n=samples_to_add, replace=True, random_state=SEED)\n    \n#     train_balanced_df = pd.concat([train_full_df, upsampled_minority]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n#     print(f\"\\nUpsampled label '{minority_label}' by adding {samples_to_add} samples.\")\n#     print(\"New balanced label distribution:\\n\", train_balanced_df['label'].value_counts())\n# else:\n#     train_balanced_df = train_full_df.copy()\n#     print(\"\\nNo upsampling needed as classes are balanced.\")\n\n# # --- Train/Validation Split (Stratified) ---\n# train_df, val_df = train_test_split(\n#     train_balanced_df[[\"text\", \"label\"]],\n#     test_size=0.20,\n#     random_state=SEED,\n#     stratify=train_balanced_df[\"label\"]\n# )\n# print(f\"\\nData split. Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n\n# # --- Convert to Hugging Face Dataset objects ---\n# train_dataset = Dataset.from_pandas(train_df)\n# val_dataset = Dataset.from_pandas(val_df)\n# test_dataset = Dataset.from_pandas(test_df[[\"id\", \"text\"]].rename(columns={\"id\": \"original_id\"}))\n\n# # ===================================================================\n# # 3. MODEL CUSTOMIZATION AND TOKENIZATION\n# # ===================================================================\n# # --- Tokenizer ---\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# # --- Custom Model Head (ANN Classifier) ---\n# class CustomANNHead(nn.Module):\n#     \"\"\"A custom classifier head with multiple layers for improved performance.\"\"\"\n#     def __init__(self, input_size, inner_dim=512, dropout_rate=0.2):\n#         super().__init__()\n#         self.fc1 = nn.Linear(input_size, inner_dim)\n#         self.activation1 = nn.ReLU()\n#         self.dropout1 = nn.Dropout(dropout_rate)\n#         self.fc2 = nn.Linear(inner_dim, inner_dim // 2)\n#         self.activation2 = nn.ReLU()\n#         self.dropout2 = nn.Dropout(dropout_rate)\n#         self.output_layer = nn.Linear(inner_dim // 2, 2) # 2 for binary classification\n\n#         # Initialize weights for better stability\n#         nn.init.xavier_uniform_(self.fc1.weight)\n#         nn.init.xavier_uniform_(self.fc2.weight)\n#         nn.init.xavier_uniform_(self.output_layer.weight)\n\n#     def forward(self, features):\n#         # The RoBERTa model outputs a sequence of hidden states.\n#         # We use the representation of the first token ([CLS]) for classification.\n#         cls_token_features = features[:, 0, :]\n        \n#         x = self.dropout1(self.activation1(self.fc1(cls_token_features)))\n#         x = self.dropout2(self.activation2(self.fc2(x)))\n#         logits = self.output_layer(x)\n#         return logits\n\n# # --- Load Base Model and Attach Custom Head ---\n# model_config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n# base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)\n\n# # Replace the original classifier with our custom, more powerful one\n# hidden_layer_size = base_model.config.hidden_size\n# base_model.classifier = CustomANNHead(hidden_layer_size)\n# base_model.config.use_cache = False # Important for training\n# print(\"\\nCustom ANN head attached to the model.\")\n\n# # --- Tokenize Datasets ---\n# def tokenize_data(batch):\n#     return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_TOKEN_LENGTH)\n\n# print(\"Tokenizing datasets...\")\n# train_dataset = train_dataset.map(tokenize_data, batched=True).rename_column(\"label\", \"labels\")\n# val_dataset = val_dataset.map(tokenize_data, batched=True).rename_column(\"label\", \"labels\")\n# test_dataset = test_dataset.map(tokenize_data, batched=True)\n\n# train_dataset.set_format(\"torch\")\n# val_dataset.set_format(\"torch\")\n# test_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'original_id'])\n\n# # ===================================================================\n# # 4. TRAINING THE MODEL\n# # ===================================================================\n# # --- Metrics Calculation ---\n# def compute_metrics(eval_predictions):\n#     logits, labels = eval_predictions\n#     probabilities = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[:, 1]\n#     roc_auc = roc_auc_score(labels, probabilities)\n#     predictions = np.argmax(logits, axis=1)\n#     accuracy = (predictions == labels).mean()\n#     return {\"roc_auc\": roc_auc, \"accuracy\": accuracy}\n\n# # --- Training Arguments ---\n# training_args = TrainingArguments(\n#     output_dir=MODEL_OUTPUT_DIR,\n#     # Use 'eval_strategy' which is the correct argument name\n#     eval_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     per_device_train_batch_size=TRAIN_BATCH_SIZE,\n#     per_device_eval_batch_size=EVAL_BATCH_SIZE,\n#     num_train_epochs=NUM_EPOCHS,\n#     learning_rate=LEARNING_RATE,\n#     weight_decay=0.01,\n#     fp16=torch.cuda.is_available(), # Enable mixed-precision training if GPU is available\n#     logging_steps=20,\n#     save_total_limit=2,\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"roc_auc\",\n#     greater_is_better=True,\n#     report_to=\"none\",\n# )\n\n# # --- Initialize Trainer ---\n# trainer = Trainer(\n#     model=base_model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     data_collator=DataCollatorWithPadding(tokenizer),\n#     compute_metrics=compute_metrics,\n#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n# )\n\n# # --- Start Training ---\n# print(\"\\nStarting model training...\")\n# start_time = time.time()\n# trainer.train()\n# end_time = time.time()\n# print(f\"Training finished in {(end_time - start_time) / 60:.2f} minutes.\")\n\n# # ===================================================================\n# # 5. EVALUATION AND VISUALIZATION\n# # ===================================================================\n# print(\"\\nEvaluating model and generating visualizations...\")\n\n# # --- Get Validation Predictions ---\n# val_predictions = trainer.predict(val_dataset)\n# val_logits = val_predictions.predictions\n# val_probs = torch.softmax(torch.from_numpy(val_logits), dim=1).numpy()[:, 1]\n# val_true_labels = val_predictions.label_ids\n# val_pred_labels = np.argmax(val_logits, axis=1)\n# final_roc_auc = roc_auc_score(val_true_labels, val_probs)\n\n# print(f\"Final Validation ROC-AUC on the best model: {final_roc_auc:.4f}\")\n\n# # --- Plot 1 & 2: ROC and Precision-Recall Curves ---\n# fpr, tpr, _ = roc_curve(val_true_labels, val_probs)\n# precision, recall, _ = precision_recall_curve(val_true_labels, val_probs)\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# ax1.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.4f})', color='darkorange')\n# ax1.plot([0, 1], [0, 1], 'k--', label='Chance')\n# ax1.set_xlabel(\"False Positive Rate\")\n# ax1.set_ylabel(\"True Positive Rate\")\n# ax1.set_title(\"Validation ROC Curve\")\n# ax1.legend()\n# ax1.grid(True, alpha=0.3)\n\n# ax2.plot(recall, precision, label='Precision-Recall Curve', color='cornflowerblue')\n# ax2.set_xlabel(\"Recall\")\n# ax2.set_ylabel(\"Precision\")\n# ax2.set_title(\"Validation Precision-Recall Curve\")\n# ax2.legend()\n# ax2.grid(True, alpha=0.3)\n# plt.tight_layout()\n# plt.savefig(os.path.join(PLOT_DIR, \"roc_pr_curves.png\"))\n# plt.show()\n\n# # --- Plot 3 & 4: Confusion Matrix and Normalized Confusion Matrix ---\n# cm = confusion_matrix(val_true_labels, val_pred_labels)\n# cm_normalized = confusion_matrix(val_true_labels, val_pred_labels, normalize='true')\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n# ConfusionMatrixDisplay(cm, display_labels=[\"Human\", \"AI/Cheating\"]).plot(ax=ax1, cmap=\"Blues\")\n# ax1.set_title(\"Validation Confusion Matrix\")\n# ConfusionMatrixDisplay(cm_normalized, display_labels=[\"Human\", \"AI/Cheating\"]).plot(ax=ax2, cmap=\"Greens\")\n# ax2.set_title(\"Normalized Confusion Matrix\")\n# plt.tight_layout()\n# plt.savefig(os.path.join(PLOT_DIR, \"confusion_matrices.png\"))\n# plt.show()\n\n# # --- Plot 5 & 6: Training/Validation Loss and ROC-AUC over Epochs ---\n# log_history = pd.DataFrame(trainer.state.log_history)\n# eval_logs = log_history[log_history['eval_loss'].notna()].copy()\n# train_logs = log_history[log_history['loss'].notna() & log_history['epoch'].notna()].copy()\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n# if not train_logs.empty and not eval_logs.empty:\n#     ax1.plot(train_logs['epoch'], train_logs['loss'], marker='o', linestyle='--', label='Training Loss')\n#     ax1.plot(eval_logs['epoch'], eval_logs['eval_loss'], marker='o', label='Validation Loss')\n#     ax1.set_xlabel(\"Epoch\")\n#     ax1.set_ylabel(\"Loss\")\n#     ax1.set_title(\"Training & Validation Loss\")\n#     ax1.legend()\n#     ax1.grid(True, alpha=0.3)\n\n#     ax2.plot(eval_logs['epoch'], eval_logs['eval_roc_auc'], marker='o', label='Validation ROC-AUC', color='teal')\n#     ax2.set_xlabel(\"Epoch\")\n#     ax2.set_ylabel(\"ROC-AUC\")\n#     ax2.set_title(\"Validation ROC-AUC per Epoch\")\n#     ax2.legend()\n#     ax2.grid(True, alpha=0.3)\n# plt.tight_layout()\n# plt.savefig(os.path.join(PLOT_DIR, \"training_history.png\"))\n# plt.show()\n\n# # ===================================================================\n# # 6. SUBMISSION FILE GENERATION\n# # ===================================================================\n# print(\"\\nPredicting on the test set for submission...\")\n# test_predictions = trainer.predict(test_dataset.remove_columns('original_id'))\n# test_logits = test_predictions.predictions\n# test_probs = torch.softmax(torch.from_numpy(test_logits), dim=1).numpy()[:, 1]\n\n# # --- Create Submission DataFrame ---\n# submission_df = pd.DataFrame({\n#     \"id\": test_dataset[\"original_id\"],\n#     \"is_cheating\": test_probs\n# })\n# submission_df.to_csv(OUTPUT_SUBMISSION_PATH, index=False)\n# print(f\"Submission file created at: {OUTPUT_SUBMISSION_PATH}\")\n# print(submission_df.head())\n\n# # --- Plot 7 & 8: Distribution of Test Predictions and Validation vs. Test ---\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n# sns.histplot(submission_df['is_cheating'], bins=50, kde=True, ax=ax1, color='purple')\n# ax1.set_title(\"Distribution of Predicted Probabilities (Test Set)\")\n# ax1.set_xlabel(\"Predicted Probability of Cheating\")\n\n# sns.kdeplot(val_probs, ax=ax2, label='Validation Predictions', fill=True)\n# sns.kdeplot(submission_df['is_cheating'], ax=ax2, label='Test Predictions', fill=True)\n# ax2.set_title(\"Validation vs. Test Prediction Distributions\")\n# ax2.set_xlabel(\"Predicted Probability\")\n# ax2.legend()\n\n# plt.tight_layout()\n# plt.savefig(os.path.join(PLOT_DIR, \"prediction_distributions.png\"))\n# plt.show()\n\n# print(\"\\nProcess complete. All plots saved in:\", PLOT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:36:19.896642Z","iopub.execute_input":"2025-11-12T19:36:19.897334Z","iopub.status.idle":"2025-11-12T19:38:24.231712Z","shell.execute_reply.started":"2025-11-12T19:36:19.897308Z","shell.execute_reply":"2025-11-12T19:38:24.230683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# FINAL SUBMISSION SCRIPT: ENSEMBLE OF MULTIPLE MODELS\n# ===================================================================\n# This script will:\n# 1. Train our primary model (Specialized RoBERTa) as before.\n# 2. Train a second, powerful transformer (DeBERTa-v3) for diversity.\n# 3. Blend the predictions of these models to create a more robust\n#    and accurate final submission.\n# ===================================================================\n\n# ===================================================================\n# 1. SETUP AND INITIAL CONFIGURATION\n# ===================================================================\nimport os\nimport random\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import (AutoTokenizer, AutoConfig,\n                            AutoModelForSequenceClassification,\n                            DataCollatorWithPadding, TrainingArguments,\n                            Trainer, EarlyStoppingCallback,\n                            logging as transformers_logging)\nfrom datasets import Dataset\nfrom scipy.stats import rankdata\n\n# --- Core Settings ---\nDATA_DIR = \"/kaggle/input/mercor-ai-detection\"\nOUTPUT_DIR = \"/kaggle/working/\"\nPLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\nos.makedirs(PLOT_DIR, exist_ok=True)\n\n# --- Model & Training Hyperparameters ---\nMODEL_1_NAME = \"fakespot-ai/roberta-base-ai-text-detection-v1\"\nMODEL_2_NAME = \"microsoft/deberta-v3-base\"\nMAX_TOKEN_LENGTH = 512\nTRAIN_BATCH_SIZE = 16\nEVAL_BATCH_SIZE = 16\nNUM_EPOCHS = 10 # Reduced slightly for faster execution of two models\nLEARNING_RATE = 2e-5\nSEED = 42\n\n# --- Reproducibility ---\ndef set_seed(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\nset_seed(SEED)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\ntransformers_logging.set_verbosity_error()\n\nprint(\"Setup Complete.\")\n\n# ===================================================================\n# 2. DATA LOADING AND PREPARATION (COMMON FOR ALL MODELS)\n# ===================================================================\n# --- Helper Functions for Text Cleaning ---\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    return \" \".join(str(text).strip().replace(\"\\r\", \" \").replace(\"\\n\", \" \").split())\n\ndef format_input_text(topic, answer):\n    return f\"TOPIC: {clean_text(topic)}\\n\\nANSWER: {clean_text(answer)}\"\n\n# --- Load DataFrames ---\ntrain_full_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n\n# --- Preprocess and Create Input Text ---\ntrain_full_df[\"text\"] = train_full_df.apply(lambda row: format_input_text(row[\"topic\"], row[\"answer\"]), axis=1)\ntrain_full_df[\"label\"] = train_full_df[\"is_cheating\"].astype(int)\ntest_df[\"text\"] = test_df.apply(lambda row: format_input_text(row.get(\"topic\", \"\"), row.get(\"answer\", \"\")), axis=1)\n\n# --- Upsampling Minority Class ---\nlabel_counts = train_full_df[\"label\"].value_counts()\nminority_label = label_counts.idxmin()\nif label_counts.min() < label_counts.max():\n    samples_to_add = label_counts.max() - label_counts.min()\n    minority_df = train_full_df[train_full_df[\"label\"] == minority_label]\n    upsampled_minority = minority_df.sample(n=samples_to_add, replace=True, random_state=SEED)\n    train_balanced_df = pd.concat([train_full_df, upsampled_minority]).sample(frac=1, random_state=SEED).reset_index(drop=True)\nelse:\n    train_balanced_df = train_full_df.copy()\n\n# --- Train/Validation Split ---\ntrain_df, val_df = train_test_split(\n    train_balanced_df[[\"text\", \"label\"]],\n    test_size=0.20,\n    random_state=SEED,\n    stratify=train_balanced_df[\"label\"]\n)\n\n# ===================================================================\n# 3. GENERIC TRAINING AND PREDICTION FUNCTION\n# ===================================================================\ndef train_and_predict(model_name, train_data, val_data, test_data):\n    \"\"\"A reusable function to train a transformer model and return test predictions.\"\"\"\n    print(f\"\\n--- Starting Training for: {model_name} ---\")\n    \n    # Convert to HF Datasets\n    train_ds = Dataset.from_pandas(train_data)\n    val_ds = Dataset.from_pandas(val_data)\n    test_ds = Dataset.from_pandas(test_data)\n\n    # Load Tokenizer and Model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    model.config.use_cache = False\n\n    # Tokenize\n    def tokenize(batch):\n        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_TOKEN_LENGTH)\n        \n    train_ds = train_ds.map(tokenize, batched=True).rename_column(\"label\", \"labels\")\n    val_ds = val_ds.map(tokenize, batched=True).rename_column(\"label\", \"labels\")\n    test_ds = test_ds.map(tokenize, batched=True)\n\n    train_ds.set_format(\"torch\")\n    val_ds.set_format(\"torch\")\n    test_ds.set_format(\"torch\", columns=['input_ids', 'attention_mask'])\n\n    def compute_metrics(p):\n        logits, labels = p\n        probs = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[:, 1]\n        return {\"roc_auc\": roc_auc_score(labels, probs)}\n\n    training_args = TrainingArguments(\n        output_dir=os.path.join(OUTPUT_DIR, f\"model_{model_name.replace('/', '_')}\"),\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=8 if \"deberta\" in model_name.lower() else TRAIN_BATCH_SIZE,\n        gradient_accumulation_steps=2 if \"deberta\" in model_name.lower() else 1,\n\n        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n        num_train_epochs=NUM_EPOCHS,\n        learning_rate=LEARNING_RATE,\n        weight_decay=0.01,\n        # fp16=torch.cuda.is_available(),\n        bf16=True if torch.cuda.is_available() else False,  # more memory-efficient precision\n\n        logging_steps=50,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"roc_auc\",\n        greater_is_better=True,\n        report_to=\"none\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n    )\n\n    trainer.train()\n\n    # Get predictions on the test set\n    test_preds = trainer.predict(test_ds)\n    test_probs = torch.softmax(torch.from_numpy(test_preds.predictions), dim=1).numpy()[:, 1]\n    \n    # Get validation score for reference\n    val_preds = trainer.predict(val_ds)\n    val_probs = torch.softmax(torch.from_numpy(val_preds.predictions), dim=1).numpy()[:, 1]\n    # NEW CODE (corrected)\n    val_score = roc_auc_score(val_ds['labels'], val_probs)\n    print(f\"--- Finished {model_name}. Validation ROC-AUC: {val_score:.4f} ---\")\n    \n    return test_probs, val_score\n\n# ===================================================================\n# 4. TRAIN MODELS AND COLLECT PREDICTIONS\n# ===================================================================\npredictions = {}\nmodel_scores = {}\n\n# Train and predict with Model 1\npreds_1, score_1 = train_and_predict(MODEL_1_NAME, train_df, val_df, test_df)\npredictions['model_1'] = preds_1\nmodel_scores['model_1'] = score_1\n\n# Train and predict with Model 2\npreds_2, score_2 = train_and_predict(MODEL_2_NAME, train_df, val_df, test_df)\npredictions['model_2'] = preds_2\nmodel_scores['model_2'] = score_2\n\n# ===================================================================\n# 5. ENSEMBLE PREDICTIONS\n# ===================================================================\nprint(\"\\n--- Ensembling Predictions ---\")\n\n# Create a DataFrame with all predictions\npred_df = pd.DataFrame(predictions)\npred_df['id'] = test_df['id']\n\n# --- Strategy 1: Weighted Averaging based on Validation Score ---\n# Give more weight to the model that performed better on the validation set.\ntotal_score = sum(model_scores.values())\nweight_1 = model_scores['model_1'] / total_score\nweight_2 = model_scores['model_2'] / total_score\n\npred_df['weighted_avg'] = (pred_df['model_1'] * weight_1) + \\\n                          (pred_df['model_2'] * weight_2)\n\nprint(f\"Model 1 Val Score: {score_1:.4f}, Assigned Weight: {weight_1:.4f}\")\nprint(f\"Model 2 Val Score: {score_2:.4f}, Assigned Weight: {weight_2:.4f}\")\n\n# --- Strategy 2: Rank Averaging ---\n# This method is robust to outliers where one model is overly confident.\npred_df['rank_1'] = rankdata(pred_df['model_1'])\npred_df['rank_2'] = rankdata(pred_df['model_2'])\npred_df['avg_rank'] = (pred_df['rank_1'] + pred_df['rank_2']) / 2\n# Scale the ranks back to a [0, 1] range\npred_df['rank_ensembled'] = (pred_df['avg_rank'] - pred_df['avg_rank'].min()) / \\\n                             (pred_df['avg_rank'].max() - pred_df['avg_rank'].min())\n\n# --- Final Blended Prediction ---\n# We will create a final blend of the weighted average and the rank average.\n# This often produces the most stable and high-scoring results.\nfinal_blend = (0.6 * pred_df['weighted_avg']) + (0.4 * pred_df['rank_ensembled'])\n\n# ===================================================================\n# 6. CREATE AND SAVE FINAL SUBMISSION\n# ===================================================================\nsubmission_df = pd.DataFrame({'id': pred_df['id'], 'is_cheating': final_blend})\nsubmission_df.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)\n\nprint(\"\\nFinal ensembled submission file created successfully!\")\nprint(submission_df.head())\n\n# --- Plotting the results for analysis ---\nplt.figure(figsize=(14, 7))\n\n# Plot 1: Distribution of Final Predictions\nsns.histplot(submission_df['is_cheating'], bins=50, kde=True, color='green')\nplt.title('Distribution of Final Ensembled Predictions on Test Set', fontsize=16)\nplt.xlabel('Predicted Probability of Cheating')\nplt.ylabel('Frequency')\nplt.savefig(os.path.join(PLOT_DIR, \"final_prediction_distribution.png\"))\nplt.show()\n\n\n# Plot 2: Correlation of Model Predictions\nplt.figure(figsize=(8, 6))\ncorrelation_matrix = pred_df[['model_1', 'model_2']].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=\".3f\")\nplt.title('Correlation Between Model Predictions', fontsize=16)\nplt.savefig(os.path.join(PLOT_DIR, \"model_correlation_heatmap.png\"))\nplt.show()\n\nprint(\"\\nProcess complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T20:00:57.785154Z","iopub.execute_input":"2025-11-12T20:00:57.785883Z","iopub.status.idle":"2025-11-12T20:04:12.848417Z","shell.execute_reply.started":"2025-11-12T20:00:57.785856Z","shell.execute_reply":"2025-11-12T20:04:12.847574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}